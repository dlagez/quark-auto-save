# !/usr/bin/env python3
# -*- coding: utf-8 -*-
# Modify: 2025-09-05
# Repo: https://github.com/Cp0204/quark_auto_save
# ConfigFile: quark_config.json
"""
new Env('å¤¸å…‹è‡ªåŠ¨è¿½æ›´');
0 8,18,20 * * * quark_auto_save.py
"""
import os
import re
import sys
import json
import time
import random
import sqlite3
import uuid
import requests
import importlib
import traceback
import urllib.parse
import base64
import copy
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from natsort import natsorted

# å…¼å®¹é’é¾™
try:
    from treelib import Tree
except:
    print("æ­£åœ¨å°è¯•è‡ªåŠ¨å®‰è£…ä¾èµ–...")
    os.system("pip3 install treelib &> /dev/null")
    from treelib import Tree


app_dir = os.path.join(os.path.dirname(__file__), "app")
if os.path.isdir(app_dir) and app_dir not in sys.path:
    sys.path.insert(0, app_dir)
try:
    from sdk.cloudsaver import CloudSaver
    from sdk.pansou import PanSou
except Exception:
    CloudSaver = None
    PanSou = None


CONFIG_DATA = {}
NOTIFYS = []
GH_PROXY = os.environ.get("GH_PROXY", "https://ghproxy.net/")
LOG_DB_PATH = os.environ.get(
    "TRANSFER_LOG_DB",
    os.path.join(os.path.dirname(__file__), "config", "transfer_logs.db"),
)
RUN_ID = None
_LOG_DB_READY = False


# å‘é€é€šçŸ¥æ¶ˆæ¯
def send_ql_notify(title, body):
    try:
        # å¯¼å…¥é€šçŸ¥æ¨¡å—
        import notify

        # å¦‚æœªé…ç½® push_config åˆ™ä½¿ç”¨é’é¾™ç¯å¢ƒé€šçŸ¥è®¾ç½®
        if CONFIG_DATA.get("push_config"):
            notify.push_config.update(CONFIG_DATA["push_config"])
            notify.push_config["CONSOLE"] = notify.push_config.get("CONSOLE", True)
        notify.send(title, body)
    except Exception as e:
        if e:
            print("å‘é€é€šçŸ¥æ¶ˆæ¯å¤±è´¥ï¼")


# æ·»åŠ æ¶ˆæ¯
def add_notify(text):
    global NOTIFYS
    NOTIFYS.append(text)
    print("ğŸ“¢", text)
    return text


def _init_log_db():
    global _LOG_DB_READY
    if _LOG_DB_READY:
        return
    try:
        log_dir = os.path.dirname(LOG_DB_PATH)
        if log_dir:
            os.makedirs(log_dir, exist_ok=True)
        with sqlite3.connect(LOG_DB_PATH) as conn:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS transfer_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    run_id TEXT,
                    task_name TEXT,
                    task_type TEXT,
                    share_url TEXT,
                    save_path TEXT,
                    status TEXT,
                    reason TEXT,
                    saved_files INTEGER,
                    saved_bytes INTEGER,
                    saved_episodes TEXT,
                    duration_ms INTEGER,
                    account TEXT,
                    details TEXT,
                    created_at TEXT
                )
                """
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_transfer_logs_run_id ON transfer_logs(run_id)"
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_transfer_logs_created_at ON transfer_logs(created_at)"
            )
            columns = {
                row[1]
                for row in conn.execute("PRAGMA table_info(transfer_logs)").fetchall()
            }
        _LOG_DB_READY = True
    except Exception as e:
        print(f"æ—¥å¿—æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")


def _safe_json_dumps(data):
    if data is None:
        return None
    try:
        return json.dumps(data, ensure_ascii=False)
    except Exception:
        return None


def _tree_log_summary(tree):
    if not tree:
        return 0, None, [], None
    saved_files = 0
    saved_bytes = 0
    files = []
    episodes = set()
    try:
        nodes = tree.all_nodes_itr()
    except Exception:
        nodes = []
    for node in nodes:
        data = node.data or {}
        if data.get("is_dir"):
            continue
        saved_files += 1
        name = data.get("file_name_re") or data.get("file_name") or ""
        for num in _find_episode_numbers(name):
            episodes.add(num)
        item = {
            "path": data.get("path"),
            "fid": data.get("fid"),
            "file_name": data.get("file_name"),
            "file_name_re": data.get("file_name_re"),
        }
        size = data.get("size")
        if isinstance(size, (int, float)):
            item["size"] = int(size)
            saved_bytes += int(size)
        files.append(item)
    episodes_sorted = sorted(episodes)
    episodes_text = ",".join(str(num) for num in episodes_sorted) if episodes_sorted else None
    return saved_files, (saved_bytes or None), files, episodes_text


def log_transfer(
    run_id,
    task_name,
    task_type,
    share_url,
    save_path,
    status,
    reason,
    saved_files,
    saved_bytes,
    saved_episodes,
    duration_ms,
    account,
    details,
    created_at,
):
    _init_log_db()
    try:
        with sqlite3.connect(LOG_DB_PATH) as conn:
            conn.execute(
                """
                INSERT INTO transfer_logs (
                    run_id,
                    task_name,
                    task_type,
                    share_url,
                    save_path,
                    status,
                    reason,
                    saved_files,
                    saved_bytes,
                    saved_episodes,
                    duration_ms,
                    account,
                    details,
                    created_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    run_id,
                    task_name,
                    task_type,
                    share_url,
                    save_path,
                    status,
                    reason,
                    saved_files,
                    saved_bytes,
                    saved_episodes,
                    duration_ms,
                    account,
                    details,
                    created_at,
                ),
            )
            conn.commit()
    except Exception as e:
        print(f"æ—¥å¿—å†™å…¥å¤±è´¥: {e}")


def _find_episode_numbers(name):
    patterns = [
        r"[Ss]\d{1,2}[Ee](\d{1,3})",
        r"\bEP?\s*(\d{1,3})\b",
        r"\u7b2c\s*(\d{1,3})\s*(\u96c6|\u671f|\u8bdd|\u56de)",
        r"^\s*(\d{1,3})(?=\s*[_\.-]?\s*(4k|2160p|1080p|720p|web|webrip|hdtv|bluray|bdrip|x264|x265)|\s*\.(mp4|mkv|mov|m4v|avi|ts))",
    ]
    numbers = []
    for pattern in patterns:
        for match in re.finditer(pattern, name, re.IGNORECASE):
            try:
                num = int(match.group(1))
            except ValueError:
                continue
            if 0 < num <= 300:
                numbers.append(num)
    return numbers


def _get_latest_episode_from_list(file_list):
    latest_episode = None
    for item in file_list:
        name = item.get("file_name", "")
        for num in _find_episode_numbers(name):
            latest_episode = num if latest_episode is None else max(latest_episode, num)
    return latest_episode


def _normalize_timestamp(ts):
    try:
        ts_val = int(ts)
    except Exception:
        return None
    if ts_val < 100000000000:
        ts_val *= 1000
    return ts_val


def _parse_start_time(task):
    value = str(task.get("updated_after", "")).strip()
    if not value:
        return None
    try:
        dt = datetime.fromisoformat(value)
    except ValueError:
        try:
            dt = datetime.strptime(value, "%Y-%m-%d")
        except ValueError:
            return None
    return int(dt.timestamp() * 1000)


def _filter_by_start_time(file_list, start_ts):
    if not start_ts:
        return file_list
    filtered = []
    for item in file_list:
        if item.get("dir"):
            filtered.append(item)
            continue
        ts_val = _normalize_timestamp(item.get("updated_at"))
        if ts_val is None or ts_val >= start_ts:
            filtered.append(item)
    return filtered


def _filter_by_recent_episodes(file_list, latest_episode, recent_episodes):
    if not latest_episode or recent_episodes <= 0:
        return file_list
    min_ep = max(1, latest_episode - recent_episodes + 1)
    filtered = []
    for item in file_list:
        if item.get("dir"):
            filtered.append(item)
            continue
        nums = _find_episode_numbers(item.get("file_name", ""))
        if any(min_ep <= n <= latest_episode for n in nums):
            filtered.append(item)
    return filtered


def _get_share_latest_episode(account, shareurl, timeout=None):
    try:
        pwd_id, passcode, pdir_fid, _ = account.extract_url(shareurl)
        get_stoken = account.get_stoken(pwd_id, passcode, timeout=timeout)
        if get_stoken.get("status") != 200:
            return None
        stoken = get_stoken["data"]["stoken"]
        share_detail = account.get_detail(
            pwd_id, stoken, pdir_fid, timeout=timeout
        )
        if share_detail.get("code") != 0:
            return None
        share_file_list = share_detail["data"].get("list", [])
        if (
            len(share_file_list) == 1
            and share_file_list[0].get("dir")
            and pdir_fid in ("", 0)
        ):
            share_file_list = account.get_detail(
                pwd_id, stoken, share_file_list[0]["fid"], timeout=timeout
            )["data"].get("list", [])
        return _get_latest_episode_from_list(share_file_list)
    except Exception:
        return None


def _get_share_recent_info(account, shareurl, recent_episodes, timeout=None):
    try:
        pwd_id, passcode, pdir_fid, _ = account.extract_url(shareurl)
        get_stoken = account.get_stoken(pwd_id, passcode, timeout=timeout)
        if get_stoken.get("status") != 200:
            return None, []
        stoken = get_stoken["data"]["stoken"]
        share_detail = account.get_detail(
            pwd_id, stoken, pdir_fid, timeout=timeout
        )
        if share_detail.get("code") != 0:
            return None, []
        share_file_list = share_detail["data"].get("list", [])
        if (
            len(share_file_list) == 1
            and share_file_list[0].get("dir")
            and pdir_fid in ("", 0)
        ):
            share_file_list = account.get_detail(
                pwd_id, stoken, share_file_list[0]["fid"], timeout=timeout
            )["data"].get("list", [])
        latest_episode = _get_latest_episode_from_list(share_file_list)
        recent_files = []
        if recent_episodes and latest_episode:
            filtered = _filter_by_recent_episodes(
                share_file_list, latest_episode, recent_episodes
            )
            for item in filtered:
                if not item.get("dir"):
                    recent_files.append(item.get("file_name", ""))
        return latest_episode, recent_files
    except Exception:
        return None, []


def _search_task_suggestions(query, source_config, deep=1):
    results = []
    net_data = source_config.get("net", {})
    cs_data = source_config.get("cloudsaver", {})
    ps_data = source_config.get("pansou", {})

    if str(net_data.get("enable", "true")).lower() != "false":
        try:
            base_url = base64.b64decode("aHR0cHM6Ly9zLjkxNzc4OC54eXo=").decode()
            response = requests.get(
                f"{base_url}/task_suggestions",
                params={"q": query, "d": deep},
                timeout=15,
            )
            data = response.json()
            if isinstance(data, list):
                results.extend(data)
            elif isinstance(data, dict):
                results.extend(data.get("data", []))
        except Exception:
            pass

    if (
        CloudSaver
        and cs_data.get("server")
        and cs_data.get("username")
        and cs_data.get("password")
    ):
        try:
            cs = CloudSaver(cs_data.get("server"))
            cs.set_auth(
                cs_data.get("username", ""),
                cs_data.get("password", ""),
                cs_data.get("token", ""),
            )
            search = cs.auto_login_search(query)
            if search.get("success"):
                if search.get("new_token"):
                    cs_data["token"] = search.get("new_token")
                results.extend(cs.clean_search_results(search.get("data")))
        except Exception:
            pass

    if PanSou and ps_data.get("server"):
        try:
            ps = PanSou(ps_data.get("server"))
            results.extend(ps.search(query, deep == 1))
        except Exception:
            pass

    results.sort(key=lambda x: x.get("datetime", ""), reverse=True)
    deduped = []
    link_array = []
    for item in results:
        url = item.get("shareurl", "")
        if url and url not in link_array:
            link_array.append(url)
            deduped.append(item)
    return deduped


def resolve_smart_task(account, task):
    taskname = task.get("taskname", "").strip()
    if not taskname:
        return None, "ä»»åŠ¡åç§°ä¸ºç©º"
    if task.get("manual_shareurl"):
        resolved = copy.deepcopy(task)
        resolved["shareurl"] = task.get("manual_shareurl", "")
        resolved["smart_latest_episode"] = task.get("manual_latest_episode")
        resolved["smart_source"] = task.get("manual_source", "")
        resolved["smart_channel"] = task.get("manual_channel", "")
        resolved["save_whole_folder"] = bool(resolved.get("save_whole_folder"))
        return resolved, None
    source_config = CONFIG_DATA.get("source", {})
    candidates = _search_task_suggestions(taskname, source_config, deep=1)
    if not candidates:
        return None, "æœªæœç´¢åˆ°ç›¸å…³åˆ†äº«"
    limit = CONFIG_DATA.get("smart_search_limit", 20)
    workers = CONFIG_DATA.get("smart_search_workers", 2)
    timeout = CONFIG_DATA.get("smart_search_timeout", 12)
    try:
        limit = int(os.environ.get("SMART_SEARCH_LIMIT", limit))
    except (TypeError, ValueError):
        limit = 20
    try:
        workers = int(os.environ.get("SMART_SEARCH_WORKERS", workers))
    except (TypeError, ValueError):
        workers = 2
    try:
        timeout = float(os.environ.get("SMART_SEARCH_TIMEOUT", timeout))
    except (TypeError, ValueError):
        timeout = 12
    limit = max(1, limit)
    workers = min(4, max(1, workers))

    try:
        recent_episodes = int(task.get("recent_episodes", 0) or 0)
    except (TypeError, ValueError):
        recent_episodes = 0

    best_item = None
    best_episode = None
    fallback_item = None
    fallback_episode = None
    candidates = candidates[:limit]
    candidate_results = []
    with ThreadPoolExecutor(max_workers=workers) as executor:
        future_to_index = {}
        for idx, item in enumerate(candidates):
            shareurl = item.get("shareurl")
            candidate_results.append(
                {
                    "shareurl": shareurl or "",
                    "source": item.get("source", ""),
                    "channel": item.get("channel", ""),
                    "datetime": item.get("datetime", ""),
                    "latest_episode": None,
                    "recent_files": [],
                }
            )
            if not shareurl:
                continue
            future = executor.submit(
                _get_share_recent_info,
                account,
                shareurl,
                recent_episodes,
                timeout,
            )
            future_to_index[future] = idx
        for future in as_completed(future_to_index):
            try:
                latest_episode, recent_files = future.result()
            except Exception:
                latest_episode = None
                recent_files = []
            idx = future_to_index[future]
            candidate_results[idx]["latest_episode"] = latest_episode
            candidate_results[idx]["recent_files"] = recent_files
            if latest_episode is None:
                continue
            item = candidates[idx]
            if fallback_episode is None or latest_episode > fallback_episode:
                fallback_episode = latest_episode
                fallback_item = item
            if recent_episodes > 0 and len(recent_files) < recent_episodes:
                continue
            if best_episode is None or latest_episode > best_episode:
                best_episode = latest_episode
                best_item = item
    if best_item is None:
        if fallback_item is None:
            return None, "æœªè§£æåˆ°æœ‰æ•ˆå‰§é›†"
        best_item = fallback_item
        best_episode = fallback_episode
    resolved = copy.deepcopy(task)
    resolved["shareurl"] = best_item.get("shareurl", "")
    resolved["smart_latest_episode"] = best_episode
    resolved["smart_source"] = best_item.get("source", "")
    resolved["smart_channel"] = best_item.get("channel", "")
    resolved["smart_candidates"] = candidate_results
    resolved["save_whole_folder"] = bool(resolved.get("save_whole_folder"))
    return resolved, None


def get_smart_candidates(account, task):
    taskname = task.get("taskname", "").strip()
    if not taskname:
        return {"taskname": taskname, "error": "ä»»åŠ¡åç§°ä¸ºç©º", "candidates": []}
    source_config = CONFIG_DATA.get("source", {})
    candidates = _search_task_suggestions(taskname, source_config, deep=1)
    if not candidates:
        return {"taskname": taskname, "error": "æœªæœç´¢åˆ°ç›¸å…³åˆ†äº«", "candidates": []}
    limit = CONFIG_DATA.get("smart_search_limit", 20)
    workers = CONFIG_DATA.get("smart_search_workers", 2)
    timeout = CONFIG_DATA.get("smart_search_timeout", 12)
    try:
        limit = int(os.environ.get("SMART_SEARCH_LIMIT", limit))
    except (TypeError, ValueError):
        limit = 20
    try:
        workers = int(os.environ.get("SMART_SEARCH_WORKERS", workers))
    except (TypeError, ValueError):
        workers = 2
    try:
        timeout = float(os.environ.get("SMART_SEARCH_TIMEOUT", timeout))
    except (TypeError, ValueError):
        timeout = 12
    limit = max(1, limit)
    workers = min(4, max(1, workers))
    try:
        recent_episodes = int(task.get("recent_episodes", 0) or 0)
    except (TypeError, ValueError):
        recent_episodes = 0

    candidates = candidates[:limit]
    candidate_results = []
    with ThreadPoolExecutor(max_workers=workers) as executor:
        future_to_index = {}
        for idx, item in enumerate(candidates):
            shareurl = item.get("shareurl")
            candidate_results.append(
                {
                    "shareurl": shareurl or "",
                    "source": item.get("source", ""),
                    "channel": item.get("channel", ""),
                    "datetime": item.get("datetime", ""),
                    "latest_episode": None,
                    "recent_files": [],
                }
            )
            if not shareurl:
                continue
            future = executor.submit(
                _get_share_recent_info,
                account,
                shareurl,
                recent_episodes,
                timeout,
            )
            future_to_index[future] = idx
        for future in as_completed(future_to_index):
            try:
                latest_episode, recent_files = future.result()
            except Exception:
                latest_episode = None
                recent_files = []
            idx = future_to_index[future]
            candidate_results[idx]["latest_episode"] = latest_episode
            candidate_results[idx]["recent_files"] = recent_files

    return {"taskname": taskname, "candidates": candidate_results}


class Config:
    # ä¸‹è½½é…ç½®
    def download_file(url, save_path):
        response = requests.get(url)
        if response.status_code == 200:
            with open(save_path, "wb") as file:
                file.write(response.content)
            return True
        else:
            return False

    # è¯»å– JSON æ–‡ä»¶å†…å®¹
    def read_json(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data

    # å°†æ•°æ®å†™å…¥ JSON æ–‡ä»¶
    def write_json(config_path, data):
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, sort_keys=False, indent=2)

    # è¯»å–CK
    def get_cookies(cookie_val):
        if isinstance(cookie_val, list):
            return cookie_val
        elif cookie_val:
            if "\n" in cookie_val:
                return cookie_val.split("\n")
            else:
                return [cookie_val]
        else:
            return False

    def load_plugins(plugins_config={}, plugins_dir="plugins"):
        PLUGIN_FLAGS = os.environ.get("PLUGIN_FLAGS", "").split(",")
        plugins_available = {}
        task_plugins_config = {}
        all_modules = [
            f.replace(".py", "") for f in os.listdir(plugins_dir) if f.endswith(".py")
        ]
        # è°ƒæ•´æ¨¡å—ä¼˜å…ˆçº§
        priority_path = os.path.join(plugins_dir, "_priority.json")
        try:
            with open(priority_path, encoding="utf-8") as f:
                priority_modules = json.load(f)
            if priority_modules:
                all_modules = [
                    module for module in priority_modules if module in all_modules
                ] + [module for module in all_modules if module not in priority_modules]
        except (FileNotFoundError, json.JSONDecodeError):
            priority_modules = []
        for module_name in all_modules:
            if f"-{module_name}" in PLUGIN_FLAGS:
                continue
            try:
                module = importlib.import_module(f"{plugins_dir}.{module_name}")
                ServerClass = getattr(module, module_name.capitalize())
                # æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦å­˜åœ¨è¯¥æ¨¡å—çš„é…ç½®
                if module_name in plugins_config:
                    plugin = ServerClass(**plugins_config[module_name])
                    plugins_available[module_name] = plugin
                else:
                    plugin = ServerClass()
                    plugins_config[module_name] = plugin.default_config
                # æ£€æŸ¥æ’ä»¶æ˜¯å¦æ”¯æŒå•ç‹¬ä»»åŠ¡é…ç½®
                if hasattr(plugin, "default_task_config"):
                    task_plugins_config[module_name] = plugin.default_task_config
            except (ImportError, AttributeError) as e:
                print(f"è½½å…¥æ¨¡å— {module_name} å¤±è´¥: {e}")
        return plugins_available, plugins_config, task_plugins_config

    def breaking_change_update(config_data):
        # ğŸ”¼ Update config v0.5.x to 0.6.0
        for task in config_data.get("tasklist", []):
            if "$TASKNAME" in task.get("replace", ""):
                task["replace"] = task["replace"].replace("$TASKNAME", "{TASKNAME}")


class MagicRename:

    magic_regex = {
        "$TV": {
            "pattern": r".*?([Ss]\d{1,2})?(?:[ç¬¬EePpXx\.\-\_\( ]{1,2}|^)(\d{1,3})(?!\d).*?\.(mp4|mkv)",
            "replace": r"\1E\2.\3",
        },
        "$BLACK_WORD": {
            "pattern": r"^(?!.*çº¯äº«)(?!.*åŠ æ›´)(?!.*è¶…å‰ä¼åˆ’)(?!.*è®­ç»ƒå®¤)(?!.*è’¸è’¸æ—¥ä¸Š).*",
            "replace": "",
        },
    }

    magic_variable = {
        "{TASKNAME}": "",
        "{I}": 1,
        "{EXT}": [r"(?<=\.)\w+$"],
        "{CHINESE}": [r"[\u4e00-\u9fa5]{2,}"],
        "{DATE}": [
            r"(18|19|20)?\d{2}[\.\-/å¹´]\d{1,2}[\.\-/æœˆ]\d{1,2}",
            r"(?<!\d)[12]\d{3}[01]?\d[0123]?\d",
            r"(?<!\d)[01]?\d[\.\-/æœˆ][0123]?\d",
        ],
        "{YEAR}": [r"(?<!\d)(18|19|20)\d{2}(?!\d)"],
        "{S}": [r"(?<=[Ss])\d{1,2}(?=[EeXx])", r"(?<=[Ss])\d{1,2}"],
        "{SXX}": [r"[Ss]\d{1,2}(?=[EeXx])", r"[Ss]\d{1,2}"],
        "{E}": [
            r"(?<=[Ss]\d\d[Ee])\d{1,3}",
            r"(?<=[Ee])\d{1,3}",
            r"(?<=[Ee][Pp])\d{1,3}",
            r"(?<=ç¬¬)\d{1,3}(?=[é›†æœŸè¯éƒ¨ç¯‡])",
            r"(?<!\d)\d{1,3}(?=[é›†æœŸè¯éƒ¨ç¯‡])",
            r"(?!.*19)(?!.*20)(?<=[\._])\d{1,3}(?=[\._])",
            r"^\d{1,3}(?=\.\w+)",
            r"(?<!\d)\d{1,3}(?!\d)(?!$)",
        ],
        "{PART}": [
            r"(?<=[é›†æœŸè¯éƒ¨ç¯‡ç¬¬])[ä¸Šä¸­ä¸‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]",
            r"[ä¸Šä¸­ä¸‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]",
        ],
        "{VER}": [r"[\u4e00-\u9fa5]+ç‰ˆ"],
    }

    priority_list = [
        "ä¸Š",
        "ä¸­",
        "ä¸‹",
        "ä¸€",
        "äºŒ",
        "ä¸‰",
        "å››",
        "äº”",
        "å…­",
        "ä¸ƒ",
        "å…«",
        "ä¹",
        "å",
        "ç™¾",
        "åƒ",
        "ä¸‡",
    ]

    def __init__(self, magic_regex={}, magic_variable={}):
        self.magic_regex.update(magic_regex)
        self.magic_variable.update(magic_variable)
        self.dir_filename_dict = {}

    def set_taskname(self, taskname):
        """è®¾ç½®ä»»åŠ¡åç§°"""
        self.magic_variable["{TASKNAME}"] = taskname

    def magic_regex_conv(self, pattern, replace):
        """é­”æ³•æ­£åˆ™åŒ¹é…"""
        keyword = pattern
        if keyword in self.magic_regex:
            pattern = self.magic_regex[keyword]["pattern"]
            if replace == "":
                replace = self.magic_regex[keyword]["replace"]
        return pattern, replace

    def sub(self, pattern, replace, file_name):
        """é­”æ³•æ­£åˆ™ã€å˜é‡æ›¿æ¢"""
        if not replace:
            return file_name
        # é¢„å¤„ç†æ›¿æ¢å˜é‡
        for key, p_list in self.magic_variable.items():
            if key in replace:
                # æ­£åˆ™ç±»æ›¿æ¢å˜é‡
                if p_list and isinstance(p_list, list):
                    for p in p_list:
                        match = re.search(p, file_name)
                        if match:
                            # åŒ¹é…æˆåŠŸï¼Œæ›¿æ¢ä¸ºåŒ¹é…åˆ°çš„å€¼
                            value = match.group()
                            # æ—¥æœŸæ ¼å¼å¤„ç†ï¼šè¡¥å…¨ã€æ ¼å¼åŒ–
                            if key == "{DATE}":
                                value = "".join(
                                    [char for char in value if char.isdigit()]
                                )
                                value = (
                                    str(datetime.now().year)[: (8 - len(value))] + value
                                )
                            replace = replace.replace(key, value)
                            break
                # éæ­£åˆ™ç±»æ›¿æ¢å˜é‡
                if key == "{TASKNAME}":
                    replace = replace.replace(key, self.magic_variable["{TASKNAME}"])
                elif key == "{SXX}" and not match:
                    replace = replace.replace(key, "S01")
                elif key == "{I}":
                    continue
                else:
                    # æ¸…ç†æœªåŒ¹é…çš„ magic_variable key
                    replace = replace.replace(key, "")
        if pattern and replace:
            file_name = re.sub(pattern, replace, file_name)
        else:
            file_name = replace
        return file_name

    def _custom_sort_key(self, name):
        """è‡ªå®šä¹‰æ’åºé”®"""
        for i, keyword in enumerate(self.priority_list):
            if keyword in name:
                name = name.replace(keyword, f"_{i:02d}_")  # æ›¿æ¢ä¸ºæ•°å­—ï¼Œæ–¹ä¾¿æ’åº
        return name

    def sort_file_list(self, file_list, dir_filename_dict={}):
        """æ–‡ä»¶åˆ—è¡¨ç»Ÿä¸€æ’åºï¼Œç»™{I+}èµ‹å€¼"""
        filename_list = [
            # å¼ºåˆ¶åŠ å…¥`æ–‡ä»¶ä¿®æ”¹æ—¶é—´`å­—æ®µä¾›æ’åºï¼Œæ•ˆæœï¼š1æ— å¯æ’åºå­—ç¬¦æ—¶åˆ™æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œ2å’Œç›®å½•å·²æœ‰æ–‡ä»¶é‡åæ—¶å§‹ç»ˆåœ¨å…¶å
            f"{f['file_name_re']}_{f['updated_at']}"
            for f in file_list
            if f.get("file_name_re") and not f["dir"]
        ]
        # print(f"filename_list_before: {filename_list}")
        dir_filename_dict = dir_filename_dict or self.dir_filename_dict
        # print(f"dir_filename_list: {dir_filename_list}")
        # åˆå¹¶ç›®å½•æ–‡ä»¶åˆ—è¡¨
        filename_list = list(set(filename_list) | set(dir_filename_dict.values()))
        filename_list = natsorted(filename_list, key=self._custom_sort_key)
        filename_index = {}
        for name in filename_list:
            if name in dir_filename_dict.values():
                continue
            i = filename_list.index(name) + 1
            while i in dir_filename_dict.keys():
                i += 1
            dir_filename_dict[i] = name
            filename_index[name] = i
        for file in file_list:
            if file.get("file_name_re"):
                if match := re.search(r"\{I+\}", file["file_name_re"]):
                    i = filename_index.get(
                        f"{file['file_name_re']}_{file['updated_at']}", 0
                    )
                    file["file_name_re"] = re.sub(
                        match.group(),
                        str(i).zfill(match.group().count("I")),
                        file["file_name_re"],
                    )

    def set_dir_file_list(self, file_list, replace):
        """è®¾ç½®ç›®å½•æ–‡ä»¶åˆ—è¡¨"""
        self.dir_filename_dict = {}
        filename_list = [f["file_name"] for f in file_list if not f["dir"]]
        filename_list.sort()
        if not filename_list:
            return
        if match := re.search(r"\{I+\}", replace):
            # ç”±æ›¿æ¢å¼è½¬æ¢åŒ¹é…å¼
            magic_i = match.group()
            pattern_i = r"\d" * magic_i.count("I")
            pattern = replace.replace(match.group(), "ğŸ”¢")
            for key, _ in self.magic_variable.items():
                if key in pattern:
                    pattern = pattern.replace(key, "ğŸ”£")
            pattern = re.sub(r"\\[0-9]+", "ğŸ”£", pattern)  # \1 \2 \3
            pattern = f"({re.escape(pattern).replace('ğŸ”£', '.*?').replace('ğŸ”¢', f')({pattern_i})(')})"
            # print(f"pattern: {pattern}")
            # è·å–èµ·å§‹ç¼–å·
            if match := re.match(pattern, filename_list[-1]):
                self.magic_variable["{I}"] = int(match.group(2))
            # ç›®å½•æ–‡ä»¶åˆ—è¡¨
            for filename in filename_list:
                if match := re.match(pattern, filename):
                    self.dir_filename_dict[int(match.group(2))] = (
                        match.group(1) + magic_i + match.group(3)
                    )
            # print(f"filename_list: {self.filename_list}")

    def is_exists(self, filename, filename_list, ignore_ext=False):
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¤„ç†å¿½ç•¥æ‰©å±•å"""
        # print(f"filename: {filename} filename_list: {filename_list}")
        if ignore_ext:
            filename = os.path.splitext(filename)[0]
            filename_list = [os.path.splitext(f)[0] for f in filename_list]
        # {I+} æ¨¡å¼ï¼Œç”¨Ié€šé…æ•°å­—åºå·
        if match := re.search(r"\{I+\}", filename):
            magic_i = match.group()
            pattern_i = r"\d" * magic_i.count("I")
            pattern = re.escape(filename).replace(re.escape(magic_i), pattern_i)
            for filename in filename_list:
                if re.match(pattern, filename):
                    return filename
            return None
        else:
            return filename if filename in filename_list else None


class Quark:
    BASE_URL = "https://drive-pc.quark.cn"
    BASE_URL_APP = "https://drive-m.quark.cn"
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) quark-cloud-drive/3.14.2 Chrome/112.0.5615.165 Electron/24.1.3.8 Safari/537.36 Channel/pckk_other_ch"

    def __init__(self, cookie="", index=0):
        self.cookie = cookie.strip()
        self.index = index + 1
        self.is_active = False
        self.nickname = ""
        self.mparam = self._match_mparam_form_cookie(cookie)
        self.savepath_fid = {"/": "0"}

    def _match_mparam_form_cookie(self, cookie):
        mparam = {}
        kps_match = re.search(r"(?<!\w)kps=([a-zA-Z0-9%+/=]+)[;&]?", cookie)
        sign_match = re.search(r"(?<!\w)sign=([a-zA-Z0-9%+/=]+)[;&]?", cookie)
        vcode_match = re.search(r"(?<!\w)vcode=([a-zA-Z0-9%+/=]+)[;&]?", cookie)
        if kps_match and sign_match and vcode_match:
            mparam = {
                "kps": kps_match.group(1).replace("%25", "%"),
                "sign": sign_match.group(1).replace("%25", "%"),
                "vcode": vcode_match.group(1).replace("%25", "%"),
            }
        return mparam

    def _send_request(self, method, url, **kwargs):
        headers = {
            "cookie": self.cookie,
            "content-type": "application/json",
            "user-agent": self.USER_AGENT,
        }
        if "headers" in kwargs:
            headers = kwargs["headers"]
            del kwargs["headers"]
        if self.mparam and "share" in url and self.BASE_URL in url:
            url = url.replace(self.BASE_URL, self.BASE_URL_APP)
            kwargs["params"].update(
                {
                    "device_model": "M2011K2C",
                    "entry": "default_clouddrive",
                    "_t_group": "0%3A_s_vp%3A1",
                    "dmn": "Mi%2B11",
                    "fr": "android",
                    "pf": "3300",
                    "bi": "35937",
                    "ve": "7.4.5.680",
                    "ss": "411x875",
                    "mi": "M2011K2C",
                    "nt": "5",
                    "nw": "0",
                    "kt": "4",
                    "pr": "ucpro",
                    "sv": "release",
                    "dt": "phone",
                    "data_from": "ucapi",
                    "kps": self.mparam.get("kps"),
                    "sign": self.mparam.get("sign"),
                    "vcode": self.mparam.get("vcode"),
                    "app": "clouddrive",
                    "kkkk": "1",
                }
            )
            del headers["cookie"]
        try:
            response = requests.request(method, url, headers=headers, **kwargs)
            # print(f"{response.text}")
            # response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸï¼Œä½†è¿”å›é200ä¹Ÿä¼šæŠ›å‡ºå¼‚å¸¸
            return response
        except Exception as e:
            print(f"_send_request error:\n{e}")
            fake_response = requests.Response()
            fake_response.status_code = 500
            fake_response._content = (
                b'{"status": 500, "code": 1, "message": "request error"}'
            )
            return fake_response

    def init(self):
        account_info = self.get_account_info()
        if account_info:
            self.is_active = True
            self.nickname = account_info["nickname"]
            return account_info
        else:
            return False

    def get_account_info(self):
        url = "https://pan.quark.cn/account/info"
        querystring = {"fr": "pc", "platform": "pc"}
        response = self._send_request("GET", url, params=querystring).json()
        if response.get("data"):
            return response["data"]
        else:
            return False

    def get_growth_info(self):
        url = f"{self.BASE_URL_APP}/1/clouddrive/capacity/growth/info"
        querystring = {
            "pr": "ucpro",
            "fr": "android",
            "kps": self.mparam.get("kps"),
            "sign": self.mparam.get("sign"),
            "vcode": self.mparam.get("vcode"),
        }
        headers = {
            "content-type": "application/json",
        }
        response = self._send_request(
            "GET", url, headers=headers, params=querystring
        ).json()
        if response.get("data"):
            return response["data"]
        else:
            return False

    def get_growth_sign(self):
        url = f"{self.BASE_URL_APP}/1/clouddrive/capacity/growth/sign"
        querystring = {
            "pr": "ucpro",
            "fr": "android",
            "kps": self.mparam.get("kps"),
            "sign": self.mparam.get("sign"),
            "vcode": self.mparam.get("vcode"),
        }
        payload = {
            "sign_cyclic": True,
        }
        headers = {
            "content-type": "application/json",
        }
        response = self._send_request(
            "POST", url, json=payload, headers=headers, params=querystring
        ).json()
        if response.get("data"):
            return True, response["data"]["sign_daily_reward"]
        else:
            return False, response["message"]

    # å¯éªŒè¯èµ„æºæ˜¯å¦å¤±æ•ˆ
    def get_stoken(self, pwd_id, passcode="", timeout=None):
        url = f"{self.BASE_URL}/1/clouddrive/share/sharepage/token"
        querystring = {"pr": "ucpro", "fr": "pc"}
        payload = {"pwd_id": pwd_id, "passcode": passcode}
        response = self._send_request(
            "POST", url, json=payload, params=querystring, timeout=timeout
        ).json()
        return response

    def get_detail(
        self,
        pwd_id,
        stoken,
        pdir_fid,
        _fetch_share=0,
        fetch_share_full_path=0,
        timeout=None,
    ):
        list_merge = []
        page = 1
        while True:
            url = f"{self.BASE_URL}/1/clouddrive/share/sharepage/detail"
            querystring = {
                "pr": "ucpro",
                "fr": "pc",
                "pwd_id": pwd_id,
                "stoken": stoken,
                "pdir_fid": pdir_fid,
                "force": "0",
                "_page": page,
                "_size": "50",
                "_fetch_banner": "0",
                "_fetch_share": _fetch_share,
                "_fetch_total": "1",
                "_sort": "file_type:asc,updated_at:desc",
                "ver": "2",
                "fetch_share_full_path": fetch_share_full_path,
            }
            response = self._send_request(
                "GET", url, params=querystring, timeout=timeout
            ).json()
            if response["code"] != 0:
                return response
            if response["data"]["list"]:
                list_merge += response["data"]["list"]
                page += 1
            else:
                break
            if len(list_merge) >= response["metadata"]["_total"]:
                break
        response["data"]["list"] = list_merge
        return response

    def get_fids(self, file_paths):
        fids = []
        while True:
            url = f"{self.BASE_URL}/1/clouddrive/file/info/path_list"
            querystring = {"pr": "ucpro", "fr": "pc"}
            payload = {"file_path": file_paths[:50], "namespace": "0"}
            response = self._send_request(
                "POST", url, json=payload, params=querystring
            ).json()
            if response["code"] == 0:
                fids += response["data"]
                file_paths = file_paths[50:]
            else:
                print(f"è·å–ç›®å½•IDï¼šå¤±è´¥, {response['message']}")
                break
            if len(file_paths) == 0:
                break
        return fids

    def ls_dir(self, pdir_fid, **kwargs):
        list_merge = []
        page = 1
        while True:
            url = f"{self.BASE_URL}/1/clouddrive/file/sort"
            querystring = {
                "pr": "ucpro",
                "fr": "pc",
                "uc_param_str": "",
                "pdir_fid": pdir_fid,
                "_page": page,
                "_size": "50",
                "_fetch_total": "1",
                "_fetch_sub_dirs": "0",
                "_sort": "file_type:asc,updated_at:desc",
                "_fetch_full_path": kwargs.get("fetch_full_path", 0),
            }
            response = self._send_request("GET", url, params=querystring).json()
            if response["code"] != 0:
                return response
            if response["data"]["list"]:
                list_merge += response["data"]["list"]
                page += 1
            else:
                break
            if len(list_merge) >= response["metadata"]["_total"]:
                break
        response["data"]["list"] = list_merge
        return response

    def save_file(self, fid_list, fid_token_list, to_pdir_fid, pwd_id, stoken):
        url = f"{self.BASE_URL}/1/clouddrive/share/sharepage/save"
        querystring = {
            "pr": "ucpro",
            "fr": "pc",
            "uc_param_str": "",
            "app": "clouddrive",
            "__dt": int(random.uniform(1, 5) * 60 * 1000),
            "__t": datetime.now().timestamp(),
        }
        payload = {
            "fid_list": fid_list,
            "fid_token_list": fid_token_list,
            "to_pdir_fid": to_pdir_fid,
            "pwd_id": pwd_id,
            "stoken": stoken,
            "pdir_fid": "0",
            "scene": "link",
        }
        response = self._send_request(
            "POST", url, json=payload, params=querystring
        ).json()
        return response

    def query_task(self, task_id):
        retry_index = 0
        while True:
            url = f"{self.BASE_URL}/1/clouddrive/task"
            querystring = {
                "pr": "ucpro",
                "fr": "pc",
                "uc_param_str": "",
                "task_id": task_id,
                "retry_index": retry_index,
                "__dt": int(random.uniform(1, 5) * 60 * 1000),
                "__t": datetime.now().timestamp(),
            }
            response = self._send_request("GET", url, params=querystring).json()
            if response["status"] != 200:
                return response
            if response["data"]["status"] == 2:
                if retry_index > 0:
                    print()
                break
            else:
                if retry_index == 0:
                    print(
                        f"æ­£åœ¨ç­‰å¾…[{response['data']['task_title']}]æ‰§è¡Œç»“æœ",
                        end="",
                        flush=True,
                    )
                else:
                    print(".", end="", flush=True)
                retry_index += 1
                time.sleep(0.500)
        return response

    def download(self, fids):
        url = f"{self.BASE_URL}/1/clouddrive/file/download"
        querystring = {"pr": "ucpro", "fr": "pc", "uc_param_str": ""}
        payload = {"fids": fids}
        response = self._send_request("POST", url, json=payload, params=querystring)
        set_cookie = response.cookies.get_dict()
        cookie_str = "; ".join([f"{key}={value}" for key, value in set_cookie.items()])
        return response.json(), cookie_str

    def mkdir(self, dir_path):
        url = f"{self.BASE_URL}/1/clouddrive/file"
        querystring = {"pr": "ucpro", "fr": "pc", "uc_param_str": ""}
        payload = {
            "pdir_fid": "0",
            "file_name": "",
            "dir_path": dir_path,
            "dir_init_lock": False,
        }
        response = self._send_request(
            "POST", url, json=payload, params=querystring
        ).json()
        return response

    def rename(self, fid, file_name):
        url = f"{self.BASE_URL}/1/clouddrive/file/rename"
        querystring = {"pr": "ucpro", "fr": "pc", "uc_param_str": ""}
        payload = {"fid": fid, "file_name": file_name}
        response = self._send_request(
            "POST", url, json=payload, params=querystring
        ).json()
        return response

    def delete(self, filelist):
        url = f"{self.BASE_URL}/1/clouddrive/file/delete"
        querystring = {"pr": "ucpro", "fr": "pc", "uc_param_str": ""}
        payload = {"action_type": 2, "filelist": filelist, "exclude_fids": []}
        response = self._send_request(
            "POST", url, json=payload, params=querystring
        ).json()
        return response

    def recycle_list(self, page=1, size=30):
        url = f"{self.BASE_URL}/1/clouddrive/file/recycle/list"
        querystring = {
            "_page": page,
            "_size": size,
            "pr": "ucpro",
            "fr": "pc",
            "uc_param_str": "",
        }
        response = self._send_request("GET", url, params=querystring).json()
        return response["data"]["list"]

    def recycle_remove(self, record_list):
        url = f"{self.BASE_URL}/1/clouddrive/file/recycle/remove"
        querystring = {"uc_param_str": "", "fr": "pc", "pr": "ucpro"}
        payload = {
            "select_mode": 2,
            "record_list": record_list,
        }
        response = self._send_request(
            "POST", url, json=payload, params=querystring
        ).json()
        return response

    # â†‘ è¯·æ±‚å‡½æ•°
    # â†“ æ“ä½œå‡½æ•°

    def extract_url(self, url):
        # pwd_id
        match_id = re.search(r"/s/(\w+)", url)
        pwd_id = match_id.group(1) if match_id else None
        # passcode
        match_pwd = re.search(r"pwd=(\w+)", url)
        passcode = match_pwd.group(1) if match_pwd else ""
        # path: fid-name
        # Legacy 20250905
        paths = []
        matches = re.findall(r"/(\w{32})-?([^/]+)?", url)
        for match in matches:
            fid = match[0]
            name = urllib.parse.unquote(match[1]).replace("*101", "-")
            paths.append({"fid": fid, "name": name})
        pdir_fid = paths[-1]["fid"] if matches else 0
        return pwd_id, passcode, pdir_fid, paths

    def update_savepath_fid(self, tasklist):
        dir_paths = [
            re.sub(r"/{2,}", "/", f"/{item['savepath']}")
            for item in tasklist
            if not item.get("enddate")
            or (
                datetime.now().date()
                <= datetime.strptime(item["enddate"], "%Y-%m-%d").date()
            )
        ]
        if not dir_paths:
            return False
        dir_paths_exist_arr = self.get_fids(dir_paths)
        dir_paths_exist = [item["file_path"] for item in dir_paths_exist_arr]
        # æ¯”è¾ƒåˆ›å»ºä¸å­˜åœ¨çš„
        dir_paths_unexist = list(set(dir_paths) - set(dir_paths_exist) - set(["/"]))
        for dir_path in dir_paths_unexist:
            mkdir_return = self.mkdir(dir_path)
            if mkdir_return["code"] == 0:
                new_dir = mkdir_return["data"]
                dir_paths_exist_arr.append(
                    {"file_path": dir_path, "fid": new_dir["fid"]}
                )
                print(f"åˆ›å»ºæ–‡ä»¶å¤¹ï¼š{dir_path}")
            else:
                print(f"åˆ›å»ºæ–‡ä»¶å¤¹ï¼š{dir_path} å¤±è´¥, {mkdir_return['message']}")
        # å‚¨å­˜ç›®æ ‡ç›®å½•çš„fid
        for dir_path in dir_paths_exist_arr:
            self.savepath_fid[dir_path["file_path"]] = dir_path["fid"]
        # print(dir_paths_exist_arr)

    def do_save_check(self, shareurl, savepath):
        try:
            pwd_id, passcode, pdir_fid, _ = self.extract_url(shareurl)
            stoken = self.get_stoken(pwd_id, passcode)["data"]["stoken"]
            share_file_list = self.get_detail(pwd_id, stoken, pdir_fid)["data"]["list"]
            print(f"è·å–åˆ†äº«: {share_file_list}")
            fid_list = [item["fid"] for item in share_file_list]
            fid_token_list = [item["share_fid_token"] for item in share_file_list]
            get_fids = self.get_fids([savepath])
            to_pdir_fid = (
                get_fids[0]["fid"] if get_fids else self.mkdir(savepath)["data"]["fid"]
            )
            save_file = self.save_file(
                fid_list, fid_token_list, to_pdir_fid, pwd_id, stoken
            )
            print(f"è½¬å­˜æ–‡ä»¶: {save_file}")
            if save_file["code"] == 0:
                task_id = save_file["data"]["task_id"]
                query_task = self.query_task(task_id)
                print(f"æŸ¥è¯¢è½¬å­˜: {query_task}")
                if query_task["code"] == 0:
                    del_list = query_task["data"]["save_as"]["save_as_top_fids"]
                    if del_list:
                        delete_return = self.delete(del_list)
                        print(f"åˆ é™¤è½¬å­˜: {delete_return}")
                        recycle_list = self.recycle_list()
                        record_id_list = [
                            item["record_id"]
                            for item in recycle_list
                            if item["fid"] in del_list
                        ]
                        recycle_remove = self.recycle_remove(record_id_list)
                        print(f"æ¸…ç†è½¬å­˜: {recycle_remove}")
                        print(f"âœ… è½¬å­˜æµ‹è¯•æˆåŠŸ")
                        return True
            print(f"âŒ è½¬å­˜æµ‹è¯•å¤±è´¥: ä¸­æ–­")
            return False
        except Exception as e:
            print(f"âŒ è½¬å­˜æµ‹è¯•å¤±è´¥: {str(e)}")
            traceback.print_exc()

    def do_save_task(self, task):
        # åˆ¤æ–­èµ„æºå¤±æ•ˆè®°å½•
        if task.get("shareurl_ban"):
            print(f"ã€Š{task['taskname']}ã€‹ï¼š{task['shareurl_ban']}")
            return

        # é“¾æ¥è½¬æ¢æ‰€éœ€å‚æ•°
        pwd_id, passcode, pdir_fid, _ = self.extract_url(task["shareurl"])

        # è·å–stokenï¼ŒåŒæ—¶å¯éªŒè¯èµ„æºæ˜¯å¦å¤±æ•ˆ
        get_stoken = self.get_stoken(pwd_id, passcode)
        if get_stoken.get("status") == 200:
            stoken = get_stoken["data"]["stoken"]
        elif get_stoken.get("status") == 500:
            print(f"è·³è¿‡ä»»åŠ¡ï¼šç½‘ç»œå¼‚å¸¸ {get_stoken.get('message')}")
            return
        else:
            message = get_stoken.get("message")
            add_notify(f"âŒã€Š{task['taskname']}ã€‹ï¼š{message}\n")
            task["shareurl_ban"] = message
            return
        # print("stoken: ", stoken)

        updated_tree = self.dir_check_and_save(task, pwd_id, stoken, pdir_fid)
        if updated_tree.size(1) > 0:
            self.do_rename(updated_tree)
            print()
            add_notify(f"âœ…ã€Š{task['taskname']}ã€‹æ·»åŠ è¿½æ›´ï¼š\n{updated_tree}")
            return updated_tree
        else:
            print(f"ä»»åŠ¡ç»“æŸï¼šæ²¡æœ‰æ–°çš„è½¬å­˜ä»»åŠ¡")
            return False

    def dir_check_and_save(self, task, pwd_id, stoken, pdir_fid="", subdir_path=""):
        tree = Tree()
        # è·å–åˆ†äº«æ–‡ä»¶åˆ—è¡¨
        share_file_list = self.get_detail(pwd_id, stoken, pdir_fid)["data"]["list"]
        # print("share_file_list: ", share_file_list)

        if not share_file_list:
            if subdir_path == "":
                task["shareurl_ban"] = "åˆ†äº«ä¸ºç©ºï¼Œæ–‡ä»¶å·²è¢«åˆ†äº«è€…åˆ é™¤"
                add_notify(f"âŒã€Š{task['taskname']}ã€‹ï¼š{task['shareurl_ban']}\n")
            return tree
        elif (
            len(share_file_list) == 1
            and share_file_list[0]["dir"]
            and subdir_path == ""
        ):  # ä»…æœ‰ä¸€ä¸ªæ–‡ä»¶å¤¹
            print("ğŸ§  è¯¥åˆ†äº«æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œè¯»å–æ–‡ä»¶å¤¹å†…åˆ—è¡¨")
            share_file_list = self.get_detail(
                pwd_id, stoken, share_file_list[0]["fid"]
            )["data"]["list"]

        # è·å–ç›®æ ‡ç›®å½•æ–‡ä»¶åˆ—è¡¨
        start_ts = _parse_start_time(task)
        save_whole_folder = task.get("save_whole_folder")
        if save_whole_folder is None:
            save_whole_folder = True
        try:
            recent_episodes = int(task.get("recent_episodes", 0) or 0)
        except (TypeError, ValueError):
            recent_episodes = 0
        if recent_episodes > 0:
            latest_episode = task.get("smart_latest_episode")
            if not latest_episode:
                latest_episode = _get_latest_episode_from_list(share_file_list)
        share_file_list = _filter_by_recent_episodes(
            share_file_list, latest_episode, recent_episodes
        )
        share_file_list = _filter_by_start_time(share_file_list, start_ts)
        if not save_whole_folder:
            share_file_list = [item for item in share_file_list if not item.get("dir")]

        savepath = re.sub(r"/{2,}", "/", f"/{task['savepath']}{subdir_path}")
        if not self.savepath_fid.get(savepath):
            if get_fids := self.get_fids([savepath]):
                self.savepath_fid[savepath] = get_fids[0]["fid"]
            else:
                print(f"âŒ ç›®å½• {savepath} fidè·å–å¤±è´¥ï¼Œè·³è¿‡è½¬å­˜")
                return tree
        to_pdir_fid = self.savepath_fid[savepath]
        dir_file_list = self.ls_dir(to_pdir_fid)["data"]["list"]
        dir_filename_list = [dir_file["file_name"] for dir_file in dir_file_list]
        # print("dir_file_list: ", dir_file_list)

        tree.create_node(
            savepath,
            pdir_fid,
            data={
                "is_dir": True,
            },
        )

        # æ–‡ä»¶å‘½åç±»
        mr = MagicRename(CONFIG_DATA.get("magic_regex", {}))
        mr.set_taskname(task["taskname"])

        # é­”æ³•æ­£åˆ™è½¬æ¢
        pattern, replace = mr.magic_regex_conv(
            task.get("pattern", ""), task.get("replace", "")
        )
        # éœ€ä¿å­˜çš„æ–‡ä»¶æ¸…å•
        need_save_list = []
        # æ·»åŠ ç¬¦åˆçš„
        for share_file in share_file_list:
            search_pattern = (
                task["update_subdir"]
                if share_file["dir"] and task.get("update_subdir")
                else pattern
            )
            # æ­£åˆ™æ–‡ä»¶ååŒ¹é…
            if re.search(search_pattern, share_file["file_name"]):
                # åˆ¤æ–­åŸæ–‡ä»¶åæ˜¯å¦å­˜åœ¨ï¼Œå¤„ç†å¿½ç•¥æ‰©å±•å
                if not mr.is_exists(
                    share_file["file_name"],
                    dir_filename_list,
                    (task.get("ignore_extension") and not share_file["dir"]),
                ):
                    # æ–‡ä»¶å¤¹ã€å­ç›®å½•æ–‡ä»¶ä¸è¿›è¡Œé‡å‘½å
                    if share_file["dir"] or subdir_path:
                        share_file["file_name_re"] = share_file["file_name"]
                        need_save_list.append(share_file)
                    else:
                        # æ›¿æ¢åçš„æ–‡ä»¶å
                        file_name_re = mr.sub(pattern, replace, share_file["file_name"])
                        # åˆ¤æ–­æ›¿æ¢åçš„æ–‡ä»¶åæ˜¯å¦å­˜åœ¨
                        if not mr.is_exists(
                            file_name_re,
                            dir_filename_list,
                            task.get("ignore_extension"),
                        ):
                            share_file["file_name_re"] = file_name_re
                            need_save_list.append(share_file)
                elif share_file["dir"]:
                    # å­˜åœ¨å¹¶æ˜¯ä¸€ä¸ªç›®å½•ï¼Œå†éå­ç›®å½•
                    if task.get("update_subdir", False) and re.search(
                        task["update_subdir"], share_file["file_name"]
                    ):
                        if task.get("update_subdir_resave_mode", False):
                            # é‡å­˜æ¨¡å¼ï¼šåˆ é™¤è¯¥ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼Œé‡æ–°è½¬å­˜
                            print(f"é‡å­˜å­ç›®å½•ï¼š{savepath}/{share_file['file_name']}")
                            # åˆ é™¤å­ç›®å½•ã€å›æ”¶ç«™ä¸­å½»åº•åˆ é™¤
                            subdir = next(
                                (
                                    f
                                    for f in dir_file_list
                                    if f["file_name"] == share_file["file_name"]
                                ),
                                None,
                            )
                            delete_return = self.delete([subdir["fid"]])
                            self.query_task(delete_return["data"]["task_id"])
                            recycle_list = self.recycle_list()
                            record_id_list = [
                                item["record_id"]
                                for item in recycle_list
                                if item["fid"] == subdir["fid"]
                            ]
                            self.recycle_remove(record_id_list)
                            # ä½œä¸ºæ–°æ–‡ä»¶æ·»åŠ åˆ°è½¬å­˜åˆ—è¡¨
                            share_file["file_name_re"] = share_file["file_name"]
                            need_save_list.append(share_file)
                        else:
                            # é€’å½’æ¨¡å¼
                            print(f"æ£€æŸ¥å­ç›®å½•ï¼š{savepath}/{share_file['file_name']}")
                            subdir_tree = self.dir_check_and_save(
                                task,
                                pwd_id,
                                stoken,
                                share_file["fid"],
                                f"{subdir_path}/{share_file['file_name']}",
                            )
                            if subdir_tree.size(1) > 0:
                                # åˆå¹¶å­ç›®å½•æ ‘
                                tree.create_node(
                                    "ğŸ“" + share_file["file_name"],
                                    share_file["fid"],
                                    parent=pdir_fid,
                                    data={
                                        "is_dir": share_file["dir"],
                                    },
                                )
                                tree.merge(share_file["fid"], subdir_tree, deep=False)
            # æŒ‡å®šæ–‡ä»¶å¼€å§‹è®¢é˜…/åˆ°è¾¾æŒ‡å®šæ–‡ä»¶ï¼ˆå«ï¼‰ç»“æŸå†é
            if share_file["fid"] == task.get("startfid", ""):
                break

        if re.search(r"\{I+\}", replace):
            mr.set_dir_file_list(dir_file_list, replace)
            mr.sort_file_list(need_save_list)

        # è½¬å­˜æ–‡ä»¶
        fid_list = [item["fid"] for item in need_save_list]
        fid_token_list = [item["share_fid_token"] for item in need_save_list]
        if fid_list:
            err_msg = None
            save_as_top_fids = []
            while fid_list:
                # åˆ†æ¬¡è½¬å­˜ï¼Œ100ä¸ª/æ¬¡ï¼Œå› query_taskè¿”å›save_as_top_fidsæœ€å¤š100
                save_file_return = self.save_file(
                    fid_list[:100], fid_token_list[:100], to_pdir_fid, pwd_id, stoken
                )
                fid_list = fid_list[100:]
                fid_token_list = fid_token_list[100:]
                if save_file_return["code"] == 0:
                    # è½¬å­˜æˆåŠŸï¼ŒæŸ¥è¯¢è½¬å­˜ç»“æœ
                    task_id = save_file_return["data"]["task_id"]
                    query_task_return = self.query_task(task_id)
                    if query_task_return["code"] == 0:
                        save_as_top_fids.extend(
                            query_task_return["data"]["save_as"]["save_as_top_fids"]
                        )
                    else:
                        err_msg = query_task_return["message"]
                else:
                    err_msg = save_file_return["message"]
                if err_msg:
                    add_notify(f"âŒã€Š{task['taskname']}ã€‹è½¬å­˜å¤±è´¥ï¼š{err_msg}\n")
            # å»ºç«‹ç›®å½•æ ‘
            if len(need_save_list) == len(save_as_top_fids):
                for index, item in enumerate(need_save_list):
                    icon = self._get_file_icon(item)
                    tree.create_node(
                        f"{icon}{item['file_name_re']}",
                        item["fid"],
                        parent=pdir_fid,
                        data={
                            "file_name": item["file_name"],
                            "file_name_re": item["file_name_re"],
                            "fid": f"{save_as_top_fids[index]}",
                            "path": f"{savepath}/{item['file_name_re']}",
                            "is_dir": item["dir"],
                            "obj_category": item.get("obj_category", ""),
                            "size": item.get("size"),
                        },
                    )
        return tree

    def do_rename(self, tree, node_id=None):
        if node_id is None:
            node_id = tree.root
        for child in tree.children(node_id):
            file = child.data
            if file.get("is_dir"):
                # self.do_rename(tree, child.identifier)
                pass
            elif file.get("file_name_re") and file["file_name_re"] != file["file_name"]:
                rename_ret = self.rename(file["fid"], file["file_name_re"])
                print(f"é‡å‘½åï¼š{file['file_name']} â†’ {file['file_name_re']}")
                if rename_ret["code"] != 0:
                    print(f"      â†‘ å¤±è´¥ï¼Œ{rename_ret['message']}")

    def _get_file_icon(self, f):
        if f.get("dir"):
            return "ğŸ“"
        ico_maps = {
            "video": "ğŸï¸",
            "image": "ğŸ–¼ï¸",
            "audio": "ğŸµ",
            "doc": "ğŸ“„",
            "archive": "ğŸ“¦",
            "default": "",
        }
        return ico_maps.get(f.get("obj_category"), "")


def verify_account(account):
    # éªŒè¯è´¦å·
    print(f"â–¶ï¸ éªŒè¯ç¬¬{account.index}ä¸ªè´¦å·")
    if "__uid" not in account.cookie:
        print(f"ğŸ’¡ ä¸å­˜åœ¨cookieå¿…è¦å‚æ•°ï¼Œåˆ¤æ–­ä¸ºä»…ç­¾åˆ°")
        return False
    else:
        account_info = account.init()
        if not account_info:
            add_notify(f"ğŸ‘¤ ç¬¬{account.index}ä¸ªè´¦å·ç™»å½•å¤±è´¥ï¼Œcookieæ— æ•ˆâŒ")
            return False
        else:
            print(f"ğŸ‘¤ è´¦å·æ˜µç§°: {account_info['nickname']}âœ…")
            return True


def format_bytes(size_bytes: int) -> str:
    units = ("B", "KB", "MB", "GB", "TB", "PB", "EB", "ZB", "YB")
    i = 0
    while size_bytes >= 1024 and i < len(units) - 1:
        size_bytes /= 1024
        i += 1
    return f"{size_bytes:.2f} {units[i]}"


def do_sign(account):
    if not account.mparam:
        print("â­ï¸ ç§»åŠ¨ç«¯å‚æ•°æœªè®¾ç½®ï¼Œè·³è¿‡ç­¾åˆ°")
        print()
        return
    # æ¯æ—¥é¢†ç©ºé—´
    growth_info = account.get_growth_info()
    if growth_info:
        growth_message = f"ğŸ’¾ {'88VIP' if growth_info['88VIP'] else 'æ™®é€šç”¨æˆ·'} æ€»ç©ºé—´ï¼š{format_bytes(growth_info['total_capacity'])}ï¼Œç­¾åˆ°ç´¯è®¡è·å¾—ï¼š{format_bytes(growth_info['cap_composition'].get('sign_reward', 0))}"
        if growth_info["cap_sign"]["sign_daily"]:
            sign_message = f"ğŸ“… ç­¾åˆ°è®°å½•: ä»Šæ—¥å·²ç­¾åˆ°+{int(growth_info['cap_sign']['sign_daily_reward']/1024/1024)}MBï¼Œè¿ç­¾è¿›åº¦({growth_info['cap_sign']['sign_progress']}/{growth_info['cap_sign']['sign_target']})âœ…"
            message = f"{sign_message}\n{growth_message}"
            print(message)
        else:
            sign, sign_return = account.get_growth_sign()
            if sign:
                sign_message = f"ğŸ“… æ‰§è¡Œç­¾åˆ°: ä»Šæ—¥ç­¾åˆ°+{int(sign_return/1024/1024)}MBï¼Œè¿ç­¾è¿›åº¦({growth_info['cap_sign']['sign_progress']+1}/{growth_info['cap_sign']['sign_target']})âœ…"
                message = f"{sign_message}\n{growth_message}"
                if (
                    str(
                        CONFIG_DATA.get("push_config", {}).get("QUARK_SIGN_NOTIFY")
                    ).lower()
                    == "false"
                    or os.environ.get("QUARK_SIGN_NOTIFY") == "false"
                ):
                    print(message)
                else:
                    message = message.replace("ä»Šæ—¥", f"[{account.nickname}]ä»Šæ—¥")
                    add_notify(message)
            else:
                print(f"ğŸ“… ç­¾åˆ°å¼‚å¸¸: {sign_return}")
    print()


def do_save(account, tasklist=None, smart_tasklist=None):
    tasklist = tasklist or []
    smart_tasklist = smart_tasklist or []
    print(f"ğŸ§© è½½å…¥æ’ä»¶")
    plugins, CONFIG_DATA["plugins"], task_plugins_config = Config.load_plugins(
        CONFIG_DATA.get("plugins", {})
    )
    print(f"è½¬å­˜è´¦å·: {account.nickname}")
    # è·å–å…¨éƒ¨ä¿å­˜ç›®å½•fid
    smart_tasklist_paths = []
    for task in smart_tasklist:
        if task.get("savepath") and "TASKNAME" in task.get("savepath", ""):
            task_copy = copy.copy(task)
            task_copy["savepath"] = task["savepath"].replace(
                "TASKNAME", task.get("taskname", "")
            )
            smart_tasklist_paths.append(task_copy)
        else:
            smart_tasklist_paths.append(task)
    account.update_savepath_fid(tasklist + smart_tasklist_paths)

    def is_time(task):
        return (
            not task.get("enddate")
            or (
                datetime.now().date()
                <= datetime.strptime(task["enddate"], "%Y-%m-%d").date()
            )
        ) and (
            "runweek" not in task
            # æ˜ŸæœŸä¸€ä¸º0ï¼Œæ˜ŸæœŸæ—¥ä¸º6
            or (datetime.today().weekday() + 1 in task.get("runweek"))
        )

    # æ‰§è¡Œä»»åŠ¡
    for index, task in enumerate(tasklist):
        task_start = time.time()
        status = "skip"
        reason = None
        saved_files = None
        saved_bytes = None
        saved_episodes = None
        details = None
        is_new_tree = None
        print()
        print(f"#{index+1}------------------")
        print(f"ä»»åŠ¡åç§°: {task['taskname']}")
        print(f"åˆ†äº«é“¾æ¥: {task['shareurl']}")
        print(f"ä¿å­˜è·¯å¾„: {task['savepath']}")
        if task.get("pattern"):
            print(f"æ­£åˆ™åŒ¹é…: {task['pattern']}")
        if task.get("replace"):
            print(f"æ­£åˆ™æ›¿æ¢: {task['replace']}")
        if task.get("update_subdir"):
            print(f"æ›´å­ç›®å½•: {task['update_subdir']}")
        if task.get("runweek") or task.get("enddate"):
            print(
                f"è¿è¡Œå‘¨æœŸ: WK{task.get('runweek',[])} ~ {task.get('enddate','forever')}"
            )
        print()
        # åˆ¤æ–­ä»»åŠ¡å‘¨æœŸ
        if not is_time(task):
            print(f"ä»»åŠ¡ä¸åœ¨è¿è¡Œå‘¨æœŸå†…ï¼Œè·³è¿‡")
            reason = "outside_schedule"
        else:
            try:
                is_new_tree = account.do_save_task(task)
            except Exception as e:
                status = "fail"
                reason = str(e)
                raise

            # è¡¥å……ä»»åŠ¡çš„æ’ä»¶é…ç½®
            def merge_dicts(a, b):
                result = a.copy()
                for key, value in b.items():
                    if (
                        key in result
                        and isinstance(result[key], dict)
                        and isinstance(value, dict)
                    ):
                        result[key] = merge_dicts(result[key], value)
                    elif key not in result:
                        result[key] = value
                return result

            task["addition"] = merge_dicts(
                task.get("addition", {}), task_plugins_config
            )
            if task.get("shareurl_ban"):
                status = "fail"
                reason = task.get("shareurl_ban")
            elif is_new_tree and hasattr(is_new_tree, "size") and is_new_tree.size(1) > 0:
                status = "success"
                saved_files, saved_bytes, files, saved_episodes = _tree_log_summary(
                    is_new_tree
                )
                details = _safe_json_dumps({"files": files})
            else:
                status = "skip"
                reason = "no_new_items"
            # è°ƒç”¨æ’ä»¶
            if is_new_tree:
                print(f"ğŸ§© è°ƒç”¨æ’ä»¶")
                for plugin_name, plugin in plugins.items():
                    if plugin.is_active:
                        task = (
                            plugin.run(task, account=account, tree=is_new_tree) or task
                        )
        duration_ms = int((time.time() - task_start) * 1000)
        log_transfer(
            RUN_ID,
            task.get("taskname", ""),
            "normal",
            task.get("shareurl", ""),
            task.get("savepath", ""),
            status,
            reason,
            saved_files,
            saved_bytes,
            saved_episodes,
            duration_ms,
            account.nickname,
            details,
            datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        )
    # Smart tasks
    for index, task in enumerate(smart_tasklist):
        task_start = time.time()
        status = "skip"
        reason = None
        saved_files = None
        saved_bytes = None
        saved_episodes = None
        details = None
        resolved_task = None
        is_new_tree = None
        print()
        print(f"#S{index+1}------------------")
        print(f"Task name: {task.get('taskname', '')}")
        print(f"Save path: {task.get('savepath', '')}")
        if task.get("pattern"):
            print(f"Pattern: {task.get('pattern')}")
        if task.get("replace"):
            print(f"Replace: {task.get('replace')}")
        if task.get("update_subdir"):
            print(f"Update subdir: {task.get('update_subdir')}")
        if task.get("runweek") or task.get("enddate"):
            print(
                f"Schedule: WK{task.get('runweek',[])} ~ {task.get('enddate','forever')}"
            )
        print()
        if not is_time(task):
            print("Outside schedule, skip")
            reason = "outside_schedule"
            duration_ms = int((time.time() - task_start) * 1000)
            log_transfer(
                RUN_ID,
                task.get("taskname", ""),
                "smart",
                task.get("shareurl", ""),
                task.get("savepath", ""),
                status,
                reason,
                saved_files,
                saved_bytes,
                saved_episodes,
                duration_ms,
                account.nickname,
                details,
                datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            )
            continue

        resolved_task, err = resolve_smart_task(account, task)
        if not resolved_task:
            print(f"Auto search failed: {err}")
            add_notify(f"Auto search failed: {task.get('taskname','')}: {err}\n")
            status = "fail"
            reason = str(err)
            duration_ms = int((time.time() - task_start) * 1000)
            log_transfer(
                RUN_ID,
                task.get("taskname", ""),
                "smart",
                task.get("shareurl", ""),
                task.get("savepath", ""),
                status,
                reason,
                saved_files,
                saved_bytes,
                saved_episodes,
                duration_ms,
                account.nickname,
                details,
                datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            )
            continue
        if resolved_task.get("savepath") and "TASKNAME" in resolved_task.get("savepath", ""):
            resolved_task["savepath"] = resolved_task["savepath"].replace(
                "TASKNAME", resolved_task.get("taskname", "")
            )

        candidates = resolved_task.get("smart_candidates", [])
        if candidates:
            print("Top results:")
            for idx, item in enumerate(candidates, 1):
                episode = item.get("latest_episode")
                episode_label = f"E{episode}" if episode is not None else "E?"
                parts = [f"{idx}."]
                if item.get("source"):
                    parts.append(item.get("source"))
                if item.get("channel"):
                    parts.append(item.get("channel"))
                parts.append(episode_label)
                if item.get("shareurl"):
                    parts.append(item.get("shareurl"))
                print(" ".join(parts))
            print()

        if resolved_task.get("smart_latest_episode") is not None:
            print(
                f"Auto search: {resolved_task.get('smart_source','')} {resolved_task.get('smart_channel','')} E{resolved_task.get('smart_latest_episode')}"
            )
        print(f"Share URL: {resolved_task.get('shareurl', '')}")

        if str(os.environ.get("SMART_TEST_ONLY", "")).lower() == "true":
            print("Test mode: search only, skip save")
            status = "skip"
            reason = "smart_test_only"
            duration_ms = int((time.time() - task_start) * 1000)
            log_transfer(
                RUN_ID,
                resolved_task.get("taskname", task.get("taskname", "")),
                "smart",
                resolved_task.get("shareurl", task.get("shareurl", "")),
                resolved_task.get("savepath", task.get("savepath", "")),
                status,
                reason,
                saved_files,
                saved_bytes,
                saved_episodes,
                duration_ms,
                account.nickname,
                details,
                datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            )
            continue

        try:
            is_new_tree = account.do_save_task(resolved_task)
        except Exception as e:
            status = "fail"
            reason = str(e)
            raise

        def merge_dicts(a, b):
            result = a.copy()
            for key, value in b.items():
                if (
                    key in result
                    and isinstance(result[key], dict)
                    and isinstance(value, dict)
                ):
                    result[key] = merge_dicts(result[key], value)
                elif key not in result:
                    result[key] = value
            return result

        resolved_task["addition"] = merge_dicts(
            resolved_task.get("addition", {}), task_plugins_config
        )
        if resolved_task.get("shareurl_ban"):
            status = "fail"
            reason = resolved_task.get("shareurl_ban")
        elif is_new_tree and hasattr(is_new_tree, "size") and is_new_tree.size(1) > 0:
            status = "success"
            saved_files, saved_bytes, files, saved_episodes = _tree_log_summary(
                is_new_tree
            )
            details = _safe_json_dumps({"files": files})
        else:
            status = "skip"
            reason = "no_new_items"
        if is_new_tree:
            print("Run plugins")
            for plugin_name, plugin in plugins.items():
                if plugin.is_active:
                    resolved_task = (
                        plugin.run(resolved_task, account=account, tree=is_new_tree)
                        or resolved_task
                    )

        duration_ms = int((time.time() - task_start) * 1000)
        log_transfer(
            RUN_ID,
            resolved_task.get("taskname", task.get("taskname", "")),
            "smart",
            resolved_task.get("shareurl", task.get("shareurl", "")),
            resolved_task.get("savepath", task.get("savepath", "")),
            status,
            reason,
            saved_files,
            saved_bytes,
            saved_episodes,
            duration_ms,
            account.nickname,
            details,
            datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        )
    print()


def main():
    global CONFIG_DATA
    start_time = datetime.now()
    global RUN_ID
    if not RUN_ID:
        RUN_ID = os.environ.get("RUN_ID", uuid.uuid4().hex)
    _init_log_db()
    print(f"===============ç¨‹åºå¼€å§‹===============")
    print(f"â° æ‰§è¡Œæ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    # è¯»å–å¯åŠ¨å‚æ•°
    config_path = sys.argv[1] if len(sys.argv) > 1 else "quark_config.json"
    # æ¨é€æµ‹è¯•
    # ä»ç¯å¢ƒå˜é‡ä¸­è·å– TASKLIST
    tasklist_from_env = []
    smart_tasklist_from_env = []
    if smart_tasklist_json := os.environ.get("SMART_TASKLIST"):
        try:
            smart_tasklist_from_env = json.loads(smart_tasklist_json)
        except Exception as e:
            print(f"SMART_TASKLIST parse failed: {e}")
    if tasklist_json := os.environ.get("TASKLIST"):
        try:
            tasklist_from_env = json.loads(tasklist_json)
        except Exception as e:
            print(f"ä»ç¯å¢ƒå˜é‡è§£æä»»åŠ¡åˆ—è¡¨å¤±è´¥ {e}")
    print(f"âš™ï¸ æ­£ä» {config_path} æ–‡ä»¶ä¸­è¯»å–é…ç½®")
    CONFIG_DATA = Config.read_json(config_path)
    Config.breaking_change_update(CONFIG_DATA)
    if not CONFIG_DATA.get("smart_tasklist"):
        CONFIG_DATA["smart_tasklist"] = []
    cookie_val = CONFIG_DATA.get("cookie")
    cookie_form_file = True
    # è·å–cookie
    cookies = Config.get_cookies(cookie_val)
    if not cookies:
        print("âŒ cookie æœªé…ç½®")
        return
    accounts = [Quark(cookie, index) for index, cookie in enumerate(cookies)]
    if str(os.environ.get("SMART_CANDIDATES_ONLY", "")).lower() == "true":
        results = []
        if smart_tasklist_from_env:
            tasklist = smart_tasklist_from_env
        else:
            tasklist = CONFIG_DATA.get("smart_tasklist", [])
        for task in tasklist:
            results.append(get_smart_candidates(accounts[0], task))
        print("__SMART_CANDIDATES__" + json.dumps(results, ensure_ascii=False))
        return
    # ç­¾åˆ°
    print(f"===============ç­¾åˆ°ä»»åŠ¡===============")
    if tasklist_from_env or smart_tasklist_from_env:
        verify_account(accounts[0])
    else:
        for account in accounts:
            verify_account(account)
            do_sign(account)
    print()
    # è½¬å­˜
    if accounts[0].is_active and cookie_form_file:
        print(f"===============è½¬å­˜ä»»åŠ¡===============")
        # ä»»åŠ¡åˆ—è¡¨
        if tasklist_from_env or smart_tasklist_from_env:
            do_save(accounts[0], tasklist_from_env, smart_tasklist_from_env)
        else:
            do_save(
                accounts[0],
                CONFIG_DATA.get("tasklist", []),
                CONFIG_DATA.get("smart_tasklist", []),
            )
        print()
    # é€šçŸ¥
    if NOTIFYS:
        notify_body = "\n".join(NOTIFYS)
        print(f"===============æ¨é€é€šçŸ¥===============")
        send_ql_notify("ã€å¤¸å…‹è‡ªåŠ¨è½¬å­˜ã€‘", notify_body)
        print()
    if cookie_form_file:
        # æ›´æ–°é…ç½®
        Config.write_json(config_path, CONFIG_DATA)

    print(f"===============ç¨‹åºç»“æŸ===============")
    duration = datetime.now() - start_time
    print(f"ğŸ˜ƒ è¿è¡Œæ—¶é•¿: {round(duration.total_seconds(), 2)}s")
    print()


if __name__ == "__main__":
    main()
